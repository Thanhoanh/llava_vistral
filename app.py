import gradio as gr
from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load m√¥ h√¨nh v√† tokenizer t·ª´ th∆∞ m·ª•c ƒë√£ hu·∫•n luy·ªán
model = AutoModelForCausalLM.from_pretrained(
    "vistral-mm",  # th∆∞ m·ª•c ch·ª©a model
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
tokenizer = AutoTokenizer.from_pretrained("vistral-mm")

# H√†m x·ª≠ l√Ω c√¢u h·ªèi
def chat_with_model(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Giao di·ªán Gradio
with gr.Blocks() as demo:
    gr.Markdown("# ü§ñ Tr·ª£ l√Ω ti·∫øng Vi·ªát Vistral-MM (Text ‚Üí Text)")

    prompt_input = gr.Textbox(label="üìù Nh·∫≠p c√¢u h·ªèi ho·∫∑c m√¥ t·∫£", placeholder="V√≠ d·ª•: Tri·ªáu ch·ª©ng c·ªßa s·ªët xu·∫•t huy·∫øt?")
    output_text = gr.Textbox(label="üìã C√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh")

    btn = gr.Button("üí¨ G·ª≠i")
    btn.click(chat_with_model, inputs=prompt_input, outputs=output_text)

# Kh·ªüi ch·∫°y server
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)
